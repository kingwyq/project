首先机器学习的通俗理解就是：将无序的数据转化为有用的信息。机器学习算法主要有以下几种应用：分类、回归、聚类、密度估计。首先分类和回归都是监督学习所能实现
的作用，而聚类和密度估计属于无监督学习的范畴。分类针对的是标称型数据而回归针对的是数值型数据，可以理解所谓的标称型数据就是数据类别是离散的，而数值型就是
连续的。
对于KNN算法的总结，KNN算法的训练思想是将待分类数据与所有的训练数据求欧氏距离。这里涉及到重要的一点就是，由于数据有很多特征（数据列的个数）不同特征所对应
的数据的数量级有时差距很大，因此，为了避免由于这种差距所导致的某一特征影响占主导地位，而使得其他特征几乎失去作用。解决这一问题的方法时数据归一化，将每个
特征所对应的数据（特征数据-最小值）/（最大值-最小值）。KNN当然可以用于分类，判断手写数字等，但是，其不会给出数据的内在含义，只是单纯的求距离，进行比较。
另外，KNN不适用于训练数据太多的情况，每次预测都需要与所有训练数据求距离，速度慢。

对于决策树算法的总结，决策树算法首先也是用于分类，但它会创造规则，即如果分类还没有确定，它会根据相应的特征继续对输入数据进行划分，直到其分类确定。这里每次
对待分类数据进行划分的特征就是规则，通过这些规则不断缩小数据分类的范围，最终得出数据分类。当然，描述完其分类原理后，就有必要说明如何构建决策树了。决策树的
构建需要不断地选择分类特征，每次选择的分类特征一定是最优的，这里涉及到如何判断选择的特征就是最优的。采用的方式是选择使信息熵增益最大的特征对当前数据集进行划分
，信息熵计算公式是-∑pi*log2(pi)，这里应该明确每一个特征对应的数据类型应该是有限的，不应该是数值型数据，否则划分子集太多，训练效果也会很差。选定某一特征
然后，用这一特征对应的所有数据类型将数据集划分为几个子集，此时每个子集都可以求信息熵，总的信息熵就是各个子集信息熵的期望，即各子集信息熵*子集所占比例求和
。按每个特征划分都求出其信息熵，最终与未划分前的数据集的信息熵比较，增益最大的即为最佳划分特征。每一步选择最优划分特征，直到特征全部用完，或者所有数据都
属于同一分类，否则递归建立子分支。该方式明显包含数据的含义，从指定划分规则就可以看出。但是，缺点是容易出现过拟合，因此，需要用到剪枝。

对于朴素贝叶斯算法的总结，对于朴素贝叶斯算法来说，他并不像前面两种算法有明确的分类，而是给出属于每种分类的概率，通过比较概率的大小最终确定分类。这里需要
用到概率论中的知识，即贝叶斯公式。针对文档分类，不同的文档可能对应不同的分类，而文档中的内容要转化为采用许多特征来表示的向量的形式。我们认为这里每个不同
的单词即为一个特征，而在文档中该单词（特征）存在则将对应的位置设置为1，不存在设置为0。以这种方式，则将文档内容转化为向量形式，这里单词根据要分类的内容确定
原则上，应该包含所有文档拥有的所有单词。先就普通模型而言，一个文档就可以表示为一个向量W，而最终要得到的就是P(Ci|W)，这里Ci表示文本类别，P(Ci|W)表示向量
（文档）属于Ci的条件概率，之所以称之为朴素贝叶斯是因为这里假设所有的特征是相互独立的，即单词之间互不影响，这在实际中是不可能的，因为某些单词总是同时出现
的。利用贝叶斯公式公式计算P(Ci|W)=P(W|Ci)*P(Ci)/P(W);这里的P(W)不随Ci的变化而变化，因此，在计算过程中可以不考虑。W=[w1,w2,...,wn],
P(W|Ci)=P(w1,w2,...,wn|Ci),由前面的描述可知朴素贝叶斯是将w1,w2,...,wn看做独立，因此，P(w1,w2,...,wn|Ci)=P(w1|Ci)*P(w2|Ci)*...*P(wn|Ci)。这里首先
涉及到第一个问题，由于是连续相乘，因此，若其中某一个概率为零，则最终结果为零，因此，这里解决方式是将每个特征的初始计数值设置为1，而将总个数设置为2，另一个
问题是多个很小的数相乘，最终结果可能是0，针对这一问题的解决方式是采用取自然对数相加来代替相乘的方式。普通模型中，在一个文档中，每个单词只要出现就是1，不出现
就是0，而在词袋模型中，每个文档中特征记录的是单词出现的次数，这种模型比普通模型效果更好。通过训练数据最终计算出每一特征的条件概率，而若要估计某一文档的分类
通过将文档转化成向量后，与条件向量相乘，根据文档包含的特征选出对应的特征的条件概率，条件概率相乘（为了避免为零，是取对数相加），最终于类别概率相乘（相加），
通过比较属于每种类别的条件概率值大小决定最终分类。

对于逻辑回归算法的总结，对于二分类的逻辑回归，可以将最终的分类结果分为0,1。某些属于0类，某些属于1类。所有的训练向量可能包含多个特征，但其最终结果都是零
或者1，怎么样办到这一点呢？首先有一个sigmoid函数该函数的函数值处于(0,1)，因此，逻辑回归的做法是将输入的向量的每个特征值*对应的回归系数，然后求和，最终
将求和后的值作为自变量输入到sigmoid函数，就会得到一个(0,1)的值。怎样才能使最终的结果很好，即属于类别1的向量，最终得到的值接近1，而属于类别0的向量最终的到
的值接近0。这里就用到极大似然估计法来确定最近似的回归系数W，极大似然就是将所有的这些概率值相乘得到对应的表达式，然后取对数，求极大值所在的点对应的变量取值
，这里经过转换后的似然函数就是一个目标函数，我们的目的是使这个函数值最大。在这里参考一篇博客，博客中采用的是给似然函数加一个负号，然后，采用梯度下降法来
求对应的W。https://blog.csdn.net/u014258807/article/details/80616647，这是博客的地址。最终，通过简单的对W求偏导就可以得到对应的更新梯度，然后，就可以
通过给梯度乘以对应的步长值，来不断的接近，使目标函数值最小。注意这里目标函数非常重要（理解！！！！！）。然后就可以以程序中的那种方式更新回归系数。当然，
最后为了进一步减少计算量，引入了随机梯度下降，即每次只用一个向量来更新回归系数，相当于将N个数据向量更新变为一个数据向量，很好理解。其中还采用步长不断减小
（随着更新次数增加而减小），这相当于在接近最优值时就应该减小步长，不然的话会导致在最优值附近“跳”。




