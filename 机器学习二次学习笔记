首先机器学习的通俗理解就是：将无序的数据转化为有用的信息。机器学习算法主要有以下几种应用：分类、回归、聚类、密度估计。首先分类和回归都是监督学习所能实现
的作用，而聚类和密度估计属于无监督学习的范畴。分类针对的是标称型数据而回归针对的是数值型数据，可以理解所谓的标称型数据就是数据类别是离散的，而数值型就是
连续的。
对于KNN算法的总结，KNN算法的训练思想是将待分类数据与所有的训练数据求欧氏距离。这里涉及到重要的一点就是，由于数据有很多特征（数据列的个数）不同特征所对应
的数据的数量级有时差距很大，因此，为了避免由于这种差距所导致的某一特征影响占主导地位，而使得其他特征几乎失去作用。解决这一问题的方法时数据归一化，将每个
特征所对应的数据（特征数据-最小值）/（最大值-最小值）。KNN当然可以用于分类，判断手写数字等，但是，其不会给出数据的内在含义，只是单纯的求距离，进行比较。
另外，KNN不适用于训练数据太多的情况，每次预测都需要与所有训练数据求距离，速度慢。

对于决策树算法的总结，决策树算法首先也是用于分类，但它会创造规则，即如果分类还没有确定，它会根据相应的特征继续对输入数据进行划分，直到其分类确定。这里每次
对待分类数据进行划分的特征就是规则，通过这些规则不断缩小数据分类的范围，最终得出数据分类。当然，描述完其分类原理后，就有必要说明如何构建决策树了。决策树的
构建需要不断地选择分类特征，每次选择的分类特征一定是最优的，这里涉及到如何判断选择的特征就是最优的。采用的方式是选择使信息熵增益最大的特征对当前数据集进行划分
，信息熵计算公式是-∑pi*log2(pi)，这里应该明确每一个特征对应的数据类型应该是有限的，不应该是数值型数据，否则划分子集太多，训练效果也会很差。选定某一特征
然后，用这一特征对应的所有数据类型将数据集划分为几个子集，此时每个子集都可以求信息熵，总的信息熵就是各个子集信息熵的期望，即各子集信息熵*子集所占比例求和
。按每个特征划分都求出其信息熵，最终与未划分前的数据集的信息熵比较，增益最大的即为最佳划分特征。每一步选择最优划分特征，直到特征全部用完，或者所有数据都
属于同一分类，否则递归建立子分支。该方式明显包含数据的含义，从指定划分规则就可以看出。但是，缺点是容易出现过拟合，因此，需要用到剪枝。
