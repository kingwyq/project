首先机器学习的通俗理解就是：将无序的数据转化为有用的信息。机器学习算法主要有以下几种应用：分类、回归、聚类、密度估计。首先分类和回归都是监督学习所能实现
的作用，而聚类和密度估计属于无监督学习的范畴。分类针对的是标称型数据而回归针对的是数值型数据，可以理解所谓的标称型数据就是数据类别是离散的，而数值型就是
连续的。
对于KNN算法的总结，KNN算法的训练思想是将待分类数据与所有的训练数据求欧氏距离。这里涉及到重要的一点就是，由于数据有很多特征（数据列的个数）不同特征所对应
的数据的数量级有时差距很大，因此，为了避免由于这种差距所导致的某一特征影响占主导地位，而使得其他特征几乎失去作用。解决这一问题的方法时数据归一化，将每个
特征所对应的数据（特征数据-最小值）/（最大值-最小值）。KNN当然可以用于分类，判断手写数字等，但是，其不会给出数据的内在含义，只是单纯的求距离，进行比较。
另外，KNN不适用于训练数据太多的情况，每次预测都需要与所有训练数据求距离，速度慢。

对于决策树算法的总结，决策树算法首先也是用于分类，但它会创造规则，即如果分类还没有确定，它会根据相应的特征继续对输入数据进行划分，直到其分类确定。这里每次
对待分类数据进行划分的特征就是规则，通过这些规则不断缩小数据分类的范围，最终得出数据分类。当然，描述完其分类原理后，就有必要说明如何构建决策树了。决策树的
构建需要不断地选择分类特征，每次选择的分类特征一定是最优的，这里涉及到如何判断选择的特征就是最优的。采用的方式是选择使信息熵增益最大的特征对当前数据集进行划分
，信息熵计算公式是-∑pi*log2(pi)，这里应该明确每一个特征对应的数据类型应该是有限的，不应该是数值型数据，否则划分子集太多，训练效果也会很差。选定某一特征
然后，用这一特征对应的所有数据类型将数据集划分为几个子集，此时每个子集都可以求信息熵，总的信息熵就是各个子集信息熵的期望，即各子集信息熵*子集所占比例求和
。按每个特征划分都求出其信息熵，最终与未划分前的数据集的信息熵比较，增益最大的即为最佳划分特征。每一步选择最优划分特征，直到特征全部用完，或者所有数据都
属于同一分类，否则递归建立子分支。该方式明显包含数据的含义，从指定划分规则就可以看出。但是，缺点是容易出现过拟合，因此，需要用到剪枝。

对于朴素贝叶斯算法的总结，对于朴素贝叶斯算法来说，他并不像前面两种算法有明确的分类，而是给出属于每种分类的概率，通过比较概率的大小最终确定分类。这里需要
用到概率论中的知识，即贝叶斯公式。针对文档分类，不同的文档可能对应不同的分类，而文档中的内容要转化为采用许多特征来表示的向量的形式。我们认为这里每个不同
的单词即为一个特征，而在文档中该单词（特征）存在则将对应的位置设置为1，不存在设置为0。以这种方式，则将文档内容转化为向量形式，这里单词根据要分类的内容确定
原则上，应该包含所有文档拥有的所有单词。先就普通模型而言，一个文档就可以表示为一个向量W，而最终要得到的就是P(Ci|W)，这里Ci表示文本类别，P(Ci|W)表示向量
（文档）属于Ci的条件概率，之所以称之为朴素贝叶斯是因为这里假设所有的特征是相互独立的，即单词之间互不影响，这在实际中是不可能的，因为某些单词总是同时出现
的。利用贝叶斯公式公式计算P(Ci|W)=P(W|Ci)*P(Ci)/P(W);这里的P(W)不随Ci的变化而变化，因此，在计算过程中可以不考虑。W=[w1,w2,...,wn],
P(W|Ci)=P(w1,w2,...,wn|Ci),由前面的描述可知朴素贝叶斯是将w1,w2,...,wn看做独立，因此，P(w1,w2,...,wn|Ci)=P(w1|Ci)*P(w2|Ci)*...*P(wn|Ci)。这里首先
涉及到第一个问题，由于是连续相乘，因此，若其中某一个概率为零，则最终结果为零，因此，这里解决方式是将每个特征的初始计数值设置为1，而将总个数设置为2，另一个
问题是多个很小的数相乘，最终结果可能是0，针对这一问题的解决方式是采用取自然对数相加来代替相乘的方式。普通模型中，在一个文档中，每个单词只要出现就是1，不出现
就是0，而在词袋模型中，每个文档中特征记录的是单词出现的次数，这种模型比普通模型效果更好。通过训练数据最终计算出每一特征的条件概率，而若要估计某一文档的分类
通过将文档转化成向量后，与条件向量相乘，根据文档包含的特征选出对应的特征的条件概率，条件概率相乘（为了避免为零，是取对数相加），最终于类别概率相乘（相加），
通过比较属于每种类别的条件概率值大小决定最终分类。

对于逻辑回归算法的总结，对于二分类的逻辑回归，可以将最终的分类结果分为0,1。某些属于0类，某些属于1类。所有的训练向量可能包含多个特征，但其最终结果都是零
或者1，怎么样办到这一点呢？首先有一个sigmoid函数该函数的函数值处于(0,1)，因此，逻辑回归的做法是将输入的向量的每个特征值*对应的回归系数，然后求和，最终
将求和后的值作为自变量输入到sigmoid函数，就会得到一个(0,1)的值。怎样才能使最终的结果很好，即属于类别1的向量，最终得到的值接近1，而属于类别0的向量最终的到
的值接近0。这里就用到极大似然估计法来确定最近似的回归系数W，极大似然就是将所有的这些概率值相乘得到对应的表达式，然后取对数，求极大值所在的点对应的变量取值
，这里经过转换后的似然函数就是一个目标函数，我们的目的是使这个函数值最大。在这里参考一篇博客，博客中采用的是给似然函数加一个负号，然后，采用梯度下降法来
求对应的W。https://blog.csdn.net/u014258807/article/details/80616647，这是博客的地址。最终，通过简单的对W求偏导就可以得到对应的更新梯度，然后，就可以
通过给梯度乘以对应的步长值，来不断的接近，使目标函数值最小。注意这里目标函数非常重要（理解！！！！！）。然后就可以以程序中的那种方式更新回归系数。当然，
最后为了进一步减少计算量，引入了随机梯度下降，即每次只用一个向量来更新回归系数，相当于将N个数据向量更新变为一个数据向量，很好理解。其中还采用步长不断减小
（随着更新次数增加而减小），这相当于在接近最优值时就应该减小步长，不然的话会导致在最优值附近“跳”。

对于Adaboosting算法的总结，对于Adaboosting算法来说，其主要思想是多个弱分类器共同作用于同一向量完成分类，每个弱分类器有对应的权值，通过加权求和得出最
终的分类。一种最简单的应用是将多个单层决策树组合使用形成强分类器。当然也可以将多个不同类别的分类器进行组合。开始将所有训练数据的权值设为相同值，通过对
每个特征，对每个特征下对应的每个域值，对二分类单层决策树来说，是分别选取大于域值分为一类，小于该域值则分为另一类。最终选取错误率最低的分类器作为本次权值
对应的最终分类器。然后将该分类器错分的数据对应的权值增大，而将其正确分类的数据对应的权值降低。然后再采用新的权值获取该权值对应的最优分类器。上述过程一直
持续下去，直到对训练数据的错误概率降为零，则退出，或者是弱分类器的个数已经达到用户设定的个数。采用该分类器测试数据（针对二分类问题）即采用每个弱分类器对应
的域值与分类逻辑（可能是大于分为一类，或者小于分为一类）将所有数据进行分类，分类结果与对应权值相乘，最终将所有分类器分类结果加权求和得到最终分类。

对于回归算法的总结，对回归算法来说，这里谈到有标准回归，局部加权回归，收缩法（包括岭回归，前向逐步回归）。首先介绍标准线性回归，在训练数据集上训练出回归
系数，具体的方法是采用使∑(yi-xi.T*w).^2最小,即平方和最小，表示成矩阵乘法就是(y-x*w).T*(y-x*w),求使表达式最小的w用到求导找极值。最终推导出
w^=(x.T*x).^-1*x.T*y。根据这一表达式就可以求出w，但是需要注意到矩阵的逆可能不存在，因此需要判断。这种方式出现的问题是可能欠拟合。解决欠拟合所采用的方法
是局部加权线性回归。这种方法是给每个训练数据一个权重值，权值是通过测试数据与训练数据的差值大小来决定的，如果距离比较远则对应的权值比较低。这里的权值矩阵
用W来表示。通过给训练数据矩阵左乘该权值矩阵W（W是一个对角矩阵）。最终的表达式只是将标准回归中的x变为W*x其他部分不变，由于采用该方法时每一个测试数据都需要
与所有训练数据比较，因此比较耗时。该方法还可能出现过拟合的现象。另外一种情况是数据特征个数大于数据个数，此时，会出现x.T*x的逆不存在，此时，解决方法是
加一个“岭”，即在x.T*x上加一个λ*I可以解决，此时，只需要在标准回归表达式上改一下将x.T*x改为x.T*x+λ*I即可。这里要注意岭回归需要将数据进行标准化。最后一个
感觉比较好理解，是设置迭代次数，每次选择一个特征，在该特征对应的回归系数上加一个很小的值或者减一个很小的值。通过比较错误率来决定是加还是减。这种方式可以
得出哪种或哪几种特征对结果影响比较小，则可以减少对这部分特征的关注。

对树回归的总结，树回归包括回归树和模型树，回归树的每个叶子节点是一个数值，但是回归树可以用于回归，也可用于分类。通过设置最小错误率误差和最小分类数据集
大小，来决定是否对当前数据集进行二分。通过循环每一个特征的每一个取值，选取使分类误差最小的特征以及域值。这里的误差大小是通过计算数据的方差得到的，最终选取
方差最小的作为最优分类条件。最终的叶子结点的值是取数据集的平均值。模型树每个节点是一组回归系数，代表拟合该组数据的最佳回归系数。本节还介绍了树剪枝方法
比较合并前后的误差决定是否合并。

对k-means算法的总结，k-means是因为他可以发现k个不同的簇，每个簇的中心采用簇中所含值得均值计算而成。相似的归为一类，因此，涉及到相似度计算方法，到底什么
样的算相似，不同应用相似度计算方法不同。工作流程：先随机确定k个点作为质心，然后将每个点分配到一个簇中，为每个点找距其最近的质心，并将其分配到该质心对应的
簇中。这步完成之后，每个簇的质心更新为该簇所有点的平均值。这里停止的条件是不在有簇再改变。接下来讲到使用后处理来提高聚类性能（避免收敛到局部最小值）。
一种后处理方法是将具有最大SSE值的簇划分成两个簇，为保持簇总数不变，可以将某两个簇进行合并，合并方法一种是将距离最近的两个质心进行合并，另一种是合并任意
两个簇计算总SSE值，最后选择SSE值增幅最小的质心。还有一种二分k均值算法，先将所有点看成一个簇，当簇的数目小于k时，对每一个簇，首先计算其总误差，然后在该
簇上进行2均值聚类，计算将该簇一分为二后的总误差，最终选择使得误差最小的那个簇进行划分操作（另一种是直接选择SSE最大的簇进行划分）。聚类也被称为无监督
分类。

