主成分分析中的降维
我们首先看第一种解释的推导，即样本点到这个超平面的距离足够近。
假设m个n维数据(x(1),x(2),...,x(m))都已经进行了中心化，即∑x(i)=0。经过投影变换后得到的新坐标系为{w1,w2,...,wn},其中w是标准正交基，
即||w||2=1,(wi).T*wj=0。
如果我们将数据从n维降到n'维，即丢弃新坐标系中的部分坐标，则新的坐标系为{w1,w2,...,wn′},样本点x(i)在n'维坐标系中的投影为：
z(i)=(z(i)1,z(i)2,...,z(i)n′).T.其中，z(i)j=(wj).T*x(i)是x(i)在低维坐标系里第j维的坐标。
如果我们用z(i)来恢复原始数据x(i),则得到的恢复数据x(i)'=∑z(i)j*wj=W*z(i),其中，W为标准正交基组成的矩阵。
现在我们考虑整个样本集，我们希望所有的样本到这个超平面的距离足够近，即最小化下式：
∑(||x(i)'−x(i)||2)^2
将这个式子进行整理，可以得到:
∑i=1m||x¯¯¯(i)−x(i)||22=∑i=1m||Wz(i)−x(i)||22=∑i=1m(Wz(i))T(Wz(i))−2∑i=1m(Wz(i))Tx(i)+∑i=1mx(i)Tx(i)=∑i=1mz(i)Tz(i)−2∑i=1mz(i)TWTx(i)+∑i=1mx(i)Tx(i)=∑i=1mz(i)Tz(i)−2∑i=1mz(i)Tz(i)+∑i=1mx(i)Tx(i)=−∑i=1mz(i)Tz(i)+∑i=1mx(i)Tx(i)=−tr(WT（∑i=1mx(i)x(i)T)W)+∑i=1mx(i)Tx(i)=−tr(WTXXTW)+∑i=1mx(i)Tx(i)(1)(2)(3)(4)(5)(6)(7)
利用拉格朗日函数可以得到
J(W)=−tr(W.T*X*X.T*W+λ(W.T*W−I))
对W求导有−X*X.T*W+λW=0, 整理下即为：
X*X.T*W=λW
这样可以更清楚的看出，W为X*X.T的n'个特征向量组成的矩阵，而λ为X*X.T的若干特征值组成的矩阵，特征值在主对角线上，其余位置为0。
当我们将数据集从n维降到n'维时，需要找到最大的n'个特征值对应的特征向量。这n'个特征向量组成的矩阵W即为我们需要的矩阵。
对于原始数据集，我们只需要用z(i)=W.T*x(i),就可以把原始数据集降维到最小投影距离的n'维数据集。
